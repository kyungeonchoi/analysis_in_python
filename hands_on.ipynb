{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/title.png\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis workflow in Python\n",
    "\n",
    "<img src=\"img/ecosystem.png\">\n",
    "\n",
    "- data delivery with `ServiceX`\n",
    "- event / column selection with `func_adl`\n",
    "- data handling with `awkward-array`\n",
    "- histogram production with `coffea`\n",
    "- histogram handling with `hist`\n",
    "- visualization with `mplhep`, `hist` & `matplotlib`\n",
    "- ROOT file handling with `uproot`\n",
    "- statistical model construction with `cabinetry`\n",
    "- statistical inference with `pyhf`\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What is ServiceX?\n",
    "\n",
    "<img src=\"img/logo_servicex.png\" width=150 height=150 />\n",
    "\n",
    "ServiceX is a <span style=\"color:red\">scalable</span> <span style=\"color:blue\">HEP event data</span> <span style=\"color:purple\">extraction</span>, <span style=\"color:orange\">transformation</span> and <span style=\"color:green\">delivery</span> system\n",
    "- <span style=\"color:blue\"> HEP event data</span>: supports various input data formats - ROOT Ntuple (CMS nanoAOD), ATLAS xAOD, future data formats\n",
    "- <span style=\"color:purple\"> Extraction</span>: user-selected column(s) with filtering\n",
    "- <span style=\"color:orange\"> Transformation</span>: transform into various formats - Awkward arrays, Apache Parquet, ROOT Ntuple\n",
    "- <span style=\"color:green\"> Delivery </span>: on-demand delivery to a user or streaming into Analysis System from a remote via Rucio or XRootD\n",
    "- <span style=\"color:red\">Scalable</span>: runs on any Kubernetes cluster, scales up workers if necessary\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example ServiceX workflow\n",
    "\n",
    "<img src=\"img/ServiceX_workflow_2.png\" width=800 />\n",
    "\n",
    "1. A user makes a ServiceX delivery request from Jupyter notebook via a REST interface\n",
    "1. ServiceX backend looks for input datasets and retrieves an input file list\n",
    "1. A relevant code is generated based on the input data format, query in func-adl, and so on\n",
    "1. Transformer pods (workers) are generated to process each file (10 pods at first and scale up if necessary)\n",
    "1. Outputs are streamed into the object store inside the Kubernetes cluster\n",
    "1. Download outputs asynchronously\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Where is ServiceX?\n",
    "\n",
    "- ServiceX is deployed on Kubernetes cluster \n",
    "    - Enough resource to scale pods \n",
    "    - Preferred to be co-located with a data center for high network bandwidth\n",
    "- Types\n",
    "    - Stand-alone: Secured by own authentication system. Web API is accessible from anywhere.\n",
    "    - Integrated into coffea-casa: Secured by CERN authentication system. Only accessible inside a coffea-casa.\n",
    "- Input data format\n",
    "    - Dedicated ServiceX deployment for each input data format: ROOT ntuple, ATLAS xAOD, CMS Run-1 AOD\n",
    "    - Single deployment for all types of input data is currently under development\n",
    "- Currently available ServiceX endpoints\n",
    "\n",
    "| Type | Collaboration | Input data format | Location | Endpoint |\n",
    "| :----: |:----: | :-----------------: | :--------: | :--------: |\n",
    "| Stand-alone | ATLAS | ROOT Ntuple | UC Analysis Facility | https://uproot-atlas.servicex.af.uchicago.edu/ |\n",
    "| Stand-alone | ATLAS |xAOD | UC Analysis Facility | https://xaod.servicex.af.uchicago.edu/ |\n",
    "| Stand-alone | ATLAS| ROOT Ntuple | SSL-River | https://uproot-atlas.servicex.ssl-hep.org/ |\n",
    "| Stand-alone | ATLAS |xAOD | SSL-River | https://xaod.servicex.ssl-hep.org/ |\n",
    "| Stand-alone | - | ROOT Ntuple | SSL-River | https://atlasopendata.servicex.ssl-hep.org/ |\n",
    "| Coffea-casa | CMS| ROOT Ntuple | UNL | https://coffea.casa/ |\n",
    "| Coffea-casa | CMS | ROOT Ntuple | UNL | https://coffea-opendata.casa |\n",
    "| Coffea-casa | ATLAS | ROOT Ntuple | UC Analysis Facility | http://coffea.af.uchicago.edu/ |\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ServiceX Client Library\n",
    "\n",
    "The \"base\" library to interact with ServiceX\n",
    "\n",
    "- Makes a request to a ServiceX backend\n",
    "- Monitors the progress of transformation\n",
    "- Download files as soon as they are ready\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.6 or higher\n",
    "- A `ServiceX` endpoint\n",
    "\n",
    "\n",
    "### Configuration file\n",
    "\n",
    "- Contains endpoint access information\n",
    "- A config file can contain multiple endpoints\n",
    "- Optionally other information (such as `cache_path`) can be placed\n",
    "- For stand-alone endpoints, a config file can be downloaded from the webpage\n",
    "- The config file can be called `servicex.yaml`, `servicex.yml`, or `.servicex`. The files are searched in that order, and all present are used.\n",
    "- Searches in the current working directory, and above, and home directory (`$HOME` on Linux and Mac, and profile directory on Windows)\n",
    "\n",
    "```\n",
    "api_endpoints:\n",
    "  - name: <your-endpoint-name>\n",
    "    endpoint: <your-endpoint>\n",
    "    token: <api-token>\n",
    "    type: uproot\n",
    "```\n",
    "\n",
    "\n",
    "### Local data cache\n",
    "\n",
    "- ServiceX requests and returned data are stored in a local temporary directory by default\n",
    "- The same query on the same dataset won't create a new request to the ServiceX endpoint\n",
    "- The cache is unbound: it will continuously fill up\n",
    "- Cache path is a temporary directory of your system, but you can (or need to) change the path from the configuration file\n",
    "\n",
    "```\n",
    "cache_path: <cache-path>\n",
    "api_endpoints:\n",
    "  - name: <your-endpoint-name>\n",
    "    endpoint: <your-endpoint>\n",
    "    token: <api-token>\n",
    "    type: uproot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a ServiceX client configuration file for the endpoint at SSL-River."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servicex.yaml\n",
    "api_endpoints:\n",
    "  - name: opendata_uproot_river\n",
    "    endpoint: https://atlasopendata.servicex.ssl-hep.org/\n",
    "    type: uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the request will be sent to the endpoint at SSL-River.\n",
    "\n",
    "For stand-alone endpoints, you can monitor status of trasnformations in the dashboard:\n",
    "[SSL-River OpenData Dashboard](https://atlasopendata.servicex.ssl-hep.org/global-dashboard)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ServiceX libraries for USER\n",
    "\n",
    "- ServiceX client lirary accepts query in the format of `qastle`\n",
    "- `qastle` is not intended to be written by a user\n",
    "- Libraries to generate `qastle` query\n",
    "  - `func-adl-servicex`\n",
    "  - `tcut-to-qastle`\n",
    "\n",
    "\n",
    "### Installation\n",
    "\n",
    "Umbrella package includes both packages at [PyPI](https://pypi.org/project/servicex-clients/)\n",
    "\n",
    "`pip install servicex-clients`\n",
    "\n",
    "*pre-installed in coffea-casa opendata environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With func-adl expression\n",
    "\n",
    "- `func-adl-servicex` utilizes func-adl expression to generate `qastle` query\n",
    "- Provides objects (`ServiceXSourceUpROOT` `ServiceXSourceXAOD` `ServiceXSourceCMSRun1AOD`) that can be used as a root of a func-adl query\n",
    "\n",
    "### Hands-on ATLAS OpenData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries relevant for the ServiceX backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicex import ServiceXDataset\n",
    "from func_adl_servicex import ServiceXSourceUpROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input dataset - ATLAS opendata available via XRootD protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_opendata = \"root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define ServiceXDataset with the input data, transformer image, and the backend name defined in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_dataset = ServiceXDataset(dataset_opendata, backend_name='opendata_uproot_river')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the Uproot dataset object with tree name in the file (`mini`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ServiceXSourceUpROOT(sx_dataset, \"mini\")\n",
    "# ds.return_qastle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to write the func-adl query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_query = ds \\\n",
    "    .Select(\"lambda event: {'lep_pt': event.lep_pt, 'lep_eta': event.lep_eta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify the output data format as Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds_query.AsPandasDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing has actually been executed yet.\n",
    "\n",
    "It's finally time to make your request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have your requested columns!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With TCut expression\n",
    "\n",
    "- `tcut-to-qastle` translates TCut syntax into `qastle` query that is accepted by ServiceX client library\n",
    "- Supported expressions\n",
    "  - Arithmetic operators: `+, -, *, /`\n",
    "  - Logical operators: `!, &&, ||`\n",
    "  - Relational and comparison operators: `==, !=, >, <, >=, <=`\n",
    "  - Ternary operator: `(A?B:C)` - has to be enclosed in parentheses\n",
    "  - Mathematical function: `sqrt`\n",
    "- Usage\n",
    "  - `qastle_query = tcut_to_qastle.translate(<tree_name>, <selected_columns>, <tcut_selection>)`\n",
    "- Limitation\n",
    "    - Only worked with input data format of ROOT Ntuple\n",
    "    - `<tcut_selection>` only works with scalar type branches (crashes if a datatype of TTree branch is `std::vector`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on ATLAS OpenData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already imported the ServiceX client library\n",
    "\n",
    "`from servicex import ServiceXDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import tcut-to-qastle library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcut_to_qastle import translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ServiceX Dataset is also defined above:\n",
    "\n",
    "`sx_dataset = ServiceXDataset(dataset_opendata, image=uproot_transformer_image, backend_name='uproot')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate qastle query using tcut-to-qastle.\n",
    "\n",
    "First argument is the tree name, and the second is the list of columns. The third is the selection we apply, but the datatype of branch is `std::vector`. So, leave it empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = translate(\"mini\", \"lep_pt, lep_eta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how the qastle query looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a request using the function provided by ServiceX client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sx_dataset.get_data_pandas_df(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can force to ignore local cache if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_dataset_woCache = ServiceXDataset(dataset_opendata, backend_name='opendata_uproot_river', ignore_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deliver as a Apache parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_parquet = sx_dataset_woCache.get_data_parquet(query)\n",
    "print(out_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "ak_arr = ak.from_parquet(out_parquet)\n",
    "print(f\"lep_pt: {ak_arr.lep_pt[0]}\")\n",
    "print(f\"lep_eta: {ak_arr.lep_eta[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ServiceX DataBinder\n",
    "\n",
    "- ServiceX client libraries are all you need to interact with ServiceX backend and handle outputs.\n",
    "- DataBinder is a python package to make your life easier when you need to run a large number of datasets (e.g. full Run-2 physics analysis).\n",
    "- It handles all your ServiceX requests and delivered data from a single configuration file.\n",
    "\n",
    "### Configuration file\n",
    "\n",
    "```\n",
    "General:\n",
    "  ServiceXBackendName: uproot\n",
    "  OutputDirectory: /path/to/output\n",
    "  OutputFormat: parquet\n",
    "  \n",
    "Sample:\n",
    "  - Name: ttH\n",
    "    RucioDID: user.kchoi:user.kchoi.sampleA, \n",
    "             user.kchoi:user.kchoi.sampleB\n",
    "    Tree: nominal\n",
    "    FuncADL: \"Select(lambda event: {'jet_e': event.jet_e, 'jet_pt': event.jet_pt})\"\n",
    "  - Name: ttW\n",
    "    RucioDID: user.kchoi:user.kchoi.sampleC\n",
    "    Tree: nominal\n",
    "    Filter: n_jet > 5 \n",
    "    Columns: jet_e, jet_pt\n",
    "```\n",
    "\n",
    "- Configuration file is in yaml format\n",
    "\n",
    "| Option for `General` | Description       | DataType |\n",
    "|:--------:|:------:|:------|\n",
    "| `ServiceXBackendName` | ServiceX backend name in your `servicex.yaml` file <br> (name should contain either `uproot` or `xAOD` to distinguish the type of transformer) | `String` |\n",
    "| `OutputDirectory` | Path to the directory for ServiceX delivered files | `String` |\n",
    "| `OutputFormat` | Output file format of ServiceX delivered data (`parquet` or `root` for `uproot` / `root` for `xaod`) | `String` |\n",
    "| `ZipROOTColumns` | Zip columns that share prefix to generate one counter branch (see detail at [uproot readthedoc](https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file)) | `Boolean` |\n",
    "| `WriteOutputDict` | Name of an ouput yaml file containing Python nested dictionary of output file paths (located in the `OutputDirectory`) | `String` |\n",
    "| `IgnoreServiceXCache` | Ignore the existing ServiceX cache and force to make ServiceX requests | `Boolean` |\n",
    "\n",
    "\n",
    "| Option for `Sample` | Description       |DataType |\n",
    "|:--------:|:------:|:------|\n",
    "| `Name`   | sample name defined by a user |`String` |\n",
    "| `RucioDID` | Rucio Dataset Id (DID) for a given sample; <br> Can be multiple DIDs separated by comma |`String` |\n",
    "| `XRootDFiles` | XRootD files (e.g. `root://`) for a given sample; <br> Can be multiple files separated by comma |`String` |\n",
    "| `Tree` | Name of the input ROOT `TTree` (`uproot` ONLY) |`String` |\n",
    "| `Filter` | Selection in the TCut syntax, e.g. `jet_pt > 10e3 && jet_eta < 2.0` (TCut ONLY) |`String` |\n",
    "| `Columns` | List of columns (or branches) to be delivered; multiple columns separately by comma (TCut ONLY) |`String` |\n",
    "| `FuncADL` | func-adl expression for a given sample (func adl ONLY) |`String` |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install servicex-databinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliver data\n",
    "\n",
    "```\n",
    "from servicex_databinder import DataBinder\n",
    "sx_db = DataBinder('<CONFIG>.yml')\n",
    "out = sx_db.deliver()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on ATLAS OpenData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a configuration file \n",
    "- ServiceX requests to SSL-River ServiceX endpoint\n",
    "- Outputs in ROOT Ntuple\n",
    "- Zip `std::vector` branches if prefix is the same\n",
    "- Write a yaml file containing the paths of output files\n",
    "- Two datasets\n",
    "- Deliver two branches without selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config_databinder_opendata.yaml\n",
    "General:\n",
    "  ServiceXBackendName: opendata_uproot_river\n",
    "  OutputDirectory: ServiceXData_atlasopendata\n",
    "  OutputFormat: root\n",
    "  ZipROOTColumns: True\n",
    "  WriteOutputDict: out_atlasopendata  \n",
    "\n",
    "Sample:\n",
    "  - Name: data\n",
    "    XRootDFiles: root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/Data/data_A.4lep.root,\n",
    "            root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/Data/data_B.4lep.root,\n",
    "            root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/Data/data_C.4lep.root,\n",
    "            root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/Data/data_D.4lep.root\n",
    "    Tree: mini\n",
    "    FuncADL: \"Select(lambda event: {'lep_pt': event.lep_pt, 'lep_eta': event.lep_eta})\"\n",
    "  - Name: ggH125_ZZ4lep\n",
    "    XRootDFiles: root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root\n",
    "    Tree: mini\n",
    "    Columns: jet_pt, jet_eta, jet_phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the package and load the configuration file you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicex_databinder import DataBinder\n",
    "sx_db = DataBinder('config_databinder_opendata.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make ServiceX delivery requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sx_db.deliver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look the output ROOT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "out_file = uproot.open(out['ggH125_ZZ4lep'][0])\n",
    "print(f\"Branches in the tree: {out_file['mini'].keys()}\")\n",
    "print(f\"Branch jet_pt: {out_file['mini']['jet_pt'].array()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "\n",
    "<img src=\"img/ecosystem.png\">\n",
    "\n",
    "- data delivery with `ServiceX` --> covered today!\n",
    "- event / column selection with `func_adl`\n",
    "- data handling with `awkward-array`\n",
    "- histogram production with `coffea`\n",
    "- histogram handling with `hist`\n",
    "- visualization with `mplhep`, `hist` & `matplotlib`\n",
    "- ROOT file handling with `uproot`\n",
    "- statistical model construction with `cabinetry`\n",
    "- statistical inference with `pyhf`\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links\n",
    "\n",
    "### Tools\n",
    "- [Readthedoc - ServiceX](https://servicex.readthedocs.io/en/latest/)\n",
    "- [Github - ServiceX Frontend](https://github.com/ssl-hep/ServiceX_frontend)\n",
    "- [Github - func-adl-servicex](https://github.com/iris-hep/func_adl_servicex)\n",
    "- [Github - tcut-to-qastle](https://github.com/ssl-hep/TCutToQastleWrapper)\n",
    "\n",
    "### Workshops\n",
    "- [IRIS-HEP AGC Tools 2021 Workshop](https://indico.cern.ch/event/1076231/)\n",
    "- [PyHEP 2021](https://indico.cern.ch/event/1019958/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
