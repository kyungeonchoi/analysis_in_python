{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/title2.png\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis workflow in Python\n",
    "\n",
    "<img src=\"img/ecosystem.png\">\n",
    "\n",
    "- data delivery with `ServiceX`\n",
    "- event / column selection with `func_adl`\n",
    "- data handling with `awkward-array`\n",
    "- histogram production with `coffea`\n",
    "- histogram handling with `hist`\n",
    "- visualization with `mplhep`, `hist` & `matplotlib`\n",
    "- ROOT file handling with `uproot`\n",
    "- statistical model construction with `cabinetry`\n",
    "- statistical inference with `pyhf`\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLAS Open Data $H\\rightarrow ZZ^\\star$ with `ServiceX`, `coffea`, `cabinetry` & `pyhf`\n",
    "\n",
    "\n",
    "Disclaimer: the material in this notebook is based on the [IRIS-HEP AGC Tools 2021 Workshop](https://indico.cern.ch/event/1076231/)\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the ATLAS Open Data for this demonstration, in particular a $H\\rightarrow ZZ^\\star$ analysis. Find more information on the [ATLAS Open Data documentation](http://opendata.atlas.cern/release/2020/documentation/physics/FL2.html) and in [ATL-OREACH-PUB-2020-001](https://cds.cern.ch/record/2707171). The datasets used are [10.7483/OPENDATA.ATLAS.2Y1T.TLGL](http://doi.org/10.7483/OPENDATA.ATLAS.2Y1T.TLGL).\n",
    "\n",
    "This notebook is meant as a **technical demonstration**. In particular, the systematic uncertainties defined are purely to demonstrate technical aspects of realistic workflows, and are not meant to be meaningful physically. The fit performed to data consequently also only demonstrate technical aspects. If you are interested about the physics of $H\\rightarrow ZZ^\\star$, check out for example the actual ATLAS cross-section measurement: [Eur. Phys. J. C 80 (2020) 942](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/HIGG-2018-29/).\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level strategy\n",
    "\n",
    "We will define which files to process, set up a query with `func_adl` to extract data provided by `ServiceX`, and use `coffea` to construct histograms.\n",
    "Those histograms will be saved with `uproot`, and then assembled into a statistical model with `cabinetry`.\n",
    "Following that, we perform statistical inference with `pyhf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "from coffea.processor import servicex\n",
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "import hist\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import pyhf\n",
    "import uproot\n",
    "\n",
    "import utils\n",
    "from utils import infofile  # contains cross-section information\n",
    "\n",
    "utils.clean_up()  # delete output from previous runs of notebook (optional)\n",
    "utils.set_logging()  # configure logging output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files to process\n",
    "\n",
    "To get started, we define which files are going to be processed in this notebook.\n",
    "We also set some information for histogramming that will be used subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = (\n",
    "    \"root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/\"\n",
    ")\n",
    "\n",
    "# labels for combinations of datasets\n",
    "z_ttbar = r\"Background $Z,t\\bar{t}$\"\n",
    "zzstar = r\"Background $ZZ^{\\star}$\"\n",
    "signal = r\"Signal ($m_H$ = 125 GeV)\"\n",
    "\n",
    "fileset = {\n",
    "    \"Data\": [\n",
    "        prefix + \"Data/data_A.4lep.root\",\n",
    "        prefix + \"Data/data_B.4lep.root\",\n",
    "        prefix + \"Data/data_C.4lep.root\",\n",
    "        prefix + \"Data/data_D.4lep.root\",\n",
    "    ],\n",
    "    z_ttbar: [\n",
    "        prefix + \"MC/mc_361106.Zee.4lep.root\",\n",
    "        prefix + \"MC/mc_361107.Zmumu.4lep.root\",\n",
    "        prefix + \"MC/mc_410000.ttbar_lep.4lep.root\",\n",
    "    ],\n",
    "    zzstar: [prefix + \"MC/mc_363490.llll.4lep.root\"],\n",
    "    signal: [\n",
    "        prefix + \"MC/mc_345060.ggH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"MC/mc_344235.VBFH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"MC/mc_341964.WH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"MC/mc_341947.ZH125_ZZ4lep.4lep.root\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# information for histograms\n",
    "bin_edge_low = 80  # 80 GeV\n",
    "bin_edge_high = 250  # 250 GeV\n",
    "num_bins = 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a query with `func_adl`\n",
    "\n",
    "We are using `func_adl` for event & column selection, and make a datasource with the query built by `get_lepton_query`.\n",
    "\n",
    "A list of all available columns in the input files can be found in the [ATLAS documentation of branches](http://opendata.atlas.cern/release/2020/documentation/datasets/dataset13.html).\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> scale factor variation, applied already at event selection stage. Imagine that this could be a calculation that requires a lot of different variables which are no longer needed downstream afterwards, so it makes sense to do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lepton_query(source: ObjectStream) -> ObjectStream:\n",
    "    \"\"\"Performs event selection: require events with exactly four leptons.\n",
    "    Also select all columns needed further downstream for processing &\n",
    "    histogram filling.\n",
    "    \"\"\"\n",
    "    return source.Where(lambda e: e.lep_n == 4).Select(\n",
    "        lambda e: {\n",
    "            \"lep_pt\": e.lep_pt,\n",
    "            \"lep_eta\": e.lep_eta,\n",
    "            \"lep_phi\": e.lep_phi,\n",
    "            \"lep_energy\": e.lep_E,\n",
    "            \"lep_charge\": e.lep_charge,\n",
    "            \"lep_typeid\": e.lep_type,\n",
    "            \"mcWeight\": e.mcWeight,\n",
    "            \"scaleFactor\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP,\n",
    "            # scale factor systematic variation example\n",
    "            \"scaleFactorUP\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP\n",
    "            * 1.1,\n",
    "            \"scaleFactorDOWN\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP\n",
    "            * 0.9,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing `ServiceX`-provided data with `coffea`\n",
    "\n",
    "Event weighting: look up cross-section from a provided utility file, and correctly normalize all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample: str) -> float:\n",
    "    \"\"\"Returns normalization weight for a given sample.\"\"\"\n",
    "    lumi = 10_000  # pb^-1\n",
    "    xsec_map = infofile.infos[sample]  # dictionary with event weighting information\n",
    "    xsec_weight = (lumi * xsec_map[\"xsec\"]) / (xsec_map[\"sumw\"] * xsec_map[\"red_eff\"])\n",
    "    return xsec_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuts to apply:\n",
    "- two opposite flavor lepton pairs (total lepton charge is 0)\n",
    "- lepton types: 4 electrons, 4 muons, or 2 electrons + 2 muons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lepton_filter(lep_charge, lep_type):\n",
    "    \"\"\"Filters leptons: sum of charges is required to be 0, and sum of lepton types 44/48/52.\n",
    "    Electrons have type 11, muons have 13, so this means 4e/4mu/2e2mu.\n",
    "    \"\"\"\n",
    "    sum_lep_charge = ak.sum(lep_charge, axis=1)\n",
    "    sum_lep_type = ak.sum(lep_type, axis=1)\n",
    "    good_lep_type = ak.any(\n",
    "        [sum_lep_type == 44, sum_lep_type == 48, sum_lep_type == 52], axis=0\n",
    "    )\n",
    "    return ak.all([sum_lep_charge == 0, good_lep_type], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the `coffea` processor. It will apply cuts, calculate the four-lepton invariant mass, and fill a histogram.\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> m4l variation, applied in the processor to remaining events. This might instead for example be the result of applying a tool performing a computationally expensive calculation, which should only be run for events where it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HZZAnalysis(servicex.Analysis):\n",
    "    \"\"\"The coffea processor used in this analysis.\"\"\"\n",
    "\n",
    "    def process(self, events):\n",
    "        # type of dataset being processed, provided via metadata (comes originally from fileset)\n",
    "        dataset_category = events.metadata[\"dataset_category\"]\n",
    "\n",
    "        # get lepton information from events\n",
    "        leptons = events.lep\n",
    "\n",
    "        # apply a cut to events, based on lepton charge and lepton type\n",
    "        events = events[lepton_filter(leptons.charge, leptons.typeid)]\n",
    "        leptons = events.lep\n",
    "\n",
    "        # calculate the 4-lepton invariant mass for each remaining event\n",
    "        # this could also be an expensive calculation using external tools\n",
    "        mllll = (\n",
    "            leptons[:, 0] + leptons[:, 1] + leptons[:, 2] + leptons[:, 3]\n",
    "        ).mass / 1000\n",
    "\n",
    "        # creat histogram holding outputs, for data just binned in m4l\n",
    "        mllllhist_data = hist.Hist.new.Reg(\n",
    "            num_bins,\n",
    "            bin_edge_low,\n",
    "            bin_edge_high,\n",
    "            name=\"mllll\",\n",
    "            label=\"$\\mathrm{m_{4l}}$ [GeV]\",\n",
    "        ).Weight()  # using weighted storage here for plotting later, but not needed\n",
    "\n",
    "        # three histogram axes for MC: m4l, category, and variation (nominal and\n",
    "        # systematic variations)\n",
    "        mllllhist_MC = (\n",
    "            hist.Hist.new.Reg(\n",
    "                num_bins,\n",
    "                bin_edge_low,\n",
    "                bin_edge_high,\n",
    "                name=\"mllll\",\n",
    "                label=\"$\\mathrm{m_{4l}}$ [GeV]\",\n",
    "            )\n",
    "            .StrCat([k for k in fileset.keys() if k != \"Data\"], name=\"dataset\")\n",
    "            .StrCat(\n",
    "                [\"nominal\", \"scaleFactorUP\", \"scaleFactorDOWN\", \"m4lUP\", \"m4lDOWN\"],\n",
    "                name=\"variation\",\n",
    "            )\n",
    "            .Weight()\n",
    "        )\n",
    "\n",
    "        if dataset_category == \"Data\":\n",
    "            # create and fill a histogram for m4l\n",
    "            mllllhist_data.fill(mllll=mllll)\n",
    "\n",
    "        else:\n",
    "            # extract the sample name from the filename to calculate x-sec weight\n",
    "            sample = re.findall(r\"mc_\\d+\\.(.+)\\.4lep\", events.metadata[\"filename\"])[0]\n",
    "            basic_weight = get_xsec_weight(sample) * events.mcWeight\n",
    "            totalWeights = basic_weight * events.scaleFactor\n",
    "\n",
    "            # calculate systematic variations for weight\n",
    "            totalWeightsUp = basic_weight * events.scaleFactorUP\n",
    "            totalWeightsDown = basic_weight * events.scaleFactorDOWN\n",
    "\n",
    "            # create and fill weighted histograms for m4l: nominal and variations\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"nominal\",\n",
    "                weight=totalWeights,\n",
    "            )\n",
    "\n",
    "            # scale factor variations\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"scaleFactorUP\",\n",
    "                weight=totalWeightsUp,\n",
    "            )\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"scaleFactorDOWN\",\n",
    "                weight=totalWeightsDown,\n",
    "            )\n",
    "\n",
    "            # variation in 4-lepton invariant mass\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll * 1.01,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"m4lUP\",\n",
    "                weight=totalWeights,\n",
    "            )\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll * 0.99,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"m4lDOWN\",\n",
    "                weight=totalWeights,\n",
    "            )\n",
    "\n",
    "        return {\"data\": mllllhist_data, \"MC\": mllllhist_MC}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing the desired histograms\n",
    "\n",
    "Putting everything together: query, datasource, processor, this function runs everything so far and collects the produced histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def produce_all_histograms():\n",
    "    \"\"\"Runs the histogram production, processing input files with ServiceX and\n",
    "    producing histograms with coffea.\n",
    "    \"\"\"\n",
    "    # create the query\n",
    "    ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"mini\", backend_name=\"uproot\")\n",
    "    ds.return_qastle = True\n",
    "    lepton_query = get_lepton_query(ds)\n",
    "\n",
    "    executor = servicex.LocalExecutor()\n",
    "    # to run with the Dask executor on coffea-casa, use the following:\n",
    "    # (please note https://github.com/CoffeaTeam/coffea/issues/611)\n",
    "    # if os.environ.get(\"LABEXTENTION_FACTORY_MODULE\") == \"coffea_casa\":\n",
    "    #     executor = servicex.DaskExecutor(client_addr=\"tls://localhost:8786\")\n",
    "\n",
    "    datasources = [\n",
    "        utils.make_datasource(fileset, ds_name, lepton_query)\n",
    "        for ds_name in fileset.keys()\n",
    "    ]\n",
    "\n",
    "    # create the analysis processor\n",
    "    analysis_processor = HZZAnalysis()\n",
    "\n",
    "    async def run_updates_stream(accumulator_stream, name):\n",
    "        \"\"\"Run to get the last item in the stream\"\"\"\n",
    "        coffea_info = None\n",
    "        try:\n",
    "            async for coffea_info in accumulator_stream:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failure while processing {name}\") from e\n",
    "        return coffea_info\n",
    "\n",
    "    all_histogram_dicts = await asyncio.gather(\n",
    "        *[\n",
    "            run_updates_stream(\n",
    "                executor.execute(analysis_processor, source),\n",
    "                source.metadata[\"dataset_category\"],\n",
    "            )\n",
    "            for source in datasources\n",
    "        ]\n",
    "    )\n",
    "    full_data_histogram = sum([h[\"data\"] for h in all_histogram_dicts])\n",
    "    full_mc_histogram = sum([h[\"MC\"] for h in all_histogram_dicts])\n",
    "    \n",
    "    return {\"data\": full_data_histogram, \"MC\": full_mc_histogram}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the function just defined: obtain data from ServiceX, run processor, gather output histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# in a notebook:\n",
    "all_histograms = await produce_all_histograms()\n",
    "\n",
    "# as a script:\n",
    "# async def produce_all_the_histograms():\n",
    "#    return await produce_all_histograms()\n",
    "#\n",
    "# all_histograms = asyncio.run(produce_all_the_histograms())\n",
    "\n",
    "print(f\"execution took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting histograms with `mplhep`, `hist` & `matplotlib`\n",
    "\n",
    "We can plot some of the histograms we just produced with `mplhep`, `hist` & `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms with mplhep & hist\n",
    "mplhep.histplot(\n",
    "    all_histograms[\"data\"], histtype=\"errorbar\", color=\"black\", label=\"Data\"\n",
    ")\n",
    "hist.Hist.plot1d(\n",
    "    all_histograms[\"MC\"][:, :, \"nominal\"],\n",
    "    stack=True,\n",
    "    histtype=\"fill\",\n",
    "    color=[\"purple\", \"red\", \"lightblue\"],\n",
    ")\n",
    "\n",
    "# plot band for MC statistical uncertainties via utility function\n",
    "# (this uses matplotlib directly)\n",
    "utils.plot_errorband(bin_edge_low, bin_edge_high, num_bins, all_histograms)\n",
    "\n",
    "# we are using a small utility function to also save the figure in .png and .pdf\n",
    "# format, you can find the produced figure in the figures/ folder\n",
    "utils.save_figure(\"m4l_stat_uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving histograms with `uproot`\n",
    "\n",
    "In order to build a statistical model, we will use `cabinetry`'s support for reading histograms to build a so-called workspace specifying the model.\n",
    "We will save the histograms we just created to disk with `uproot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"histograms.root\"\n",
    "with uproot.recreate(file_name) as f:\n",
    "    f[\"data\"] = all_histograms[\"data\"]\n",
    "\n",
    "    f[\"Z_tt\"] = all_histograms[\"MC\"][:, z_ttbar, \"nominal\"]\n",
    "    f[\"Z_tt_SF_up\"] = all_histograms[\"MC\"][:, z_ttbar, \"scaleFactorUP\"]\n",
    "    f[\"Z_tt_SF_down\"] = all_histograms[\"MC\"][:, z_ttbar, \"scaleFactorDOWN\"]\n",
    "    f[\"Z_tt_m4l_up\"] = all_histograms[\"MC\"][:, z_ttbar, \"m4lUP\"]\n",
    "    f[\"Z_tt_m4l_down\"] = all_histograms[\"MC\"][:, z_ttbar, \"m4lDOWN\"]\n",
    "\n",
    "    f[\"ZZ\"] = all_histograms[\"MC\"][:, zzstar, \"nominal\"]\n",
    "    f[\"ZZ_SF_up\"] = all_histograms[\"MC\"][:, zzstar, \"scaleFactorUP\"]\n",
    "    f[\"ZZ_SF_down\"] = all_histograms[\"MC\"][:, zzstar, \"scaleFactorDOWN\"]\n",
    "    f[\"ZZ_m4l_up\"] = all_histograms[\"MC\"][:, zzstar, \"m4lUP\"]\n",
    "    f[\"ZZ_m4l_down\"] = all_histograms[\"MC\"][:, zzstar, \"m4lDOWN\"]\n",
    "\n",
    "    f[\"signal\"] = all_histograms[\"MC\"][:, signal, \"nominal\"]\n",
    "    f[\"signal_SF_up\"] = all_histograms[\"MC\"][:, signal, \"scaleFactorUP\"]\n",
    "    f[\"signal_SF_down\"] = all_histograms[\"MC\"][:, signal, \"scaleFactorDOWN\"]\n",
    "    f[\"signal_m4l_up\"] = all_histograms[\"MC\"][:, signal, \"m4lUP\"]\n",
    "    f[\"signal_m4l_down\"] = all_histograms[\"MC\"][:, signal, \"m4lDOWN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a workspace and running a fit with `cabinetry` & `pyhf`\n",
    "\n",
    "Take a look at the `cabinetry` configuration file in `config.yml`.\n",
    "It specifies the model to be built.\n",
    "In particular, it lists the samples we are going to be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"config.yml\")\n",
    "config[\"Samples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also shows which systematic uncertainties we will apply.\n",
    "This includes a new systematic uncertainty defined at this stage.\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> $ZZ^\\star$ normalization; this does not require any histograms, so we can define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"Systematics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information in the configuration is used to construct a statistical model, the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `pyhf` model and extract the data from the workspace. Perform a MLE fit, the results will be reported in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the pulls and correlations.\n",
    "`cabinetry` saves this figure by default as a `.pdf`, but here we will use our small utility again to save in both `.png` and `.pdf` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"Signal_norm\", close_figure=False, save_figure=False\n",
    ")\n",
    "utils.save_figure(\"pulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.visualize.correlation_matrix(\n",
    "    fit_results, pruning_threshold=0.15, close_figure=False, save_figure=False\n",
    ")\n",
    "utils.save_figure(\"correlation_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, visualize the post-fit model and data.\n",
    "We first create the post-fit model prediction, using the model and the best-fit resuts.\n",
    "\n",
    "The visualization is using information stored in the workspace, which does not include binning or which observable is used.\n",
    "This information can be passed in via the `config` kwarg, but we can also edit the figure after its creation.\n",
    "We will demonstrate both approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create post-fit model prediction\n",
    "postfit_model = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "\n",
    "# binning to use in plot\n",
    "plot_config = {\n",
    "    \"Regions\": [\n",
    "        {\n",
    "            \"Name\": \"Signal_region\",\n",
    "            \"Binning\": list(np.linspace(bin_edge_low, bin_edge_high, num_bins + 1)),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "figure_dict = cabinetry.visualize.data_mc(\n",
    "    postfit_model, data, config=plot_config, save_figure=False\n",
    ")\n",
    "\n",
    "# modify x-axis label\n",
    "fig = figure_dict[0][\"figure\"]\n",
    "fig.axes[1].set_xlabel(\"m4l [GeV]\")\n",
    "\n",
    "# let's also save the figure\n",
    "utils.save_figure(\"Signal_region_postfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/AGCworkshop.png\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Indico link to the IRIS-HEP AGC Tools 2022 Workshop](https://indico.cern.ch/event/1126109/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
